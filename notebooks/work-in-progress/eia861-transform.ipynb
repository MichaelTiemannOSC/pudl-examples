{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Catalyst Cooperative Jupyter Notebook Template\n",
    "This notebook lays out a standard format and some best practices for creating interactive / exploratory notebooks which can be relatively easily shared between different PUDL users, and turned into reusable Python modules for integration into our underlying Python packages.\n",
    "\n",
    "## Begin with a narrative outline\n",
    "Each notebook should start with a brief explanation (in Markdown) explaining the purpose of the analysis, and outlining the different stages / steps which are taken to accomplish the analysis.\n",
    "As the analysis develops, you can add new sections or details to this section.\n",
    "\n",
    "## Notebooks should be runnable\n",
    "Insofar as it's possible, another PUDL user who has cloned the repository that the notebook is part of should be able to update their `pudl-dev` conda environment, open the notebook, and run all cells successfully.\n",
    "If there are required data or other prerequisites that the notebook cannot manage on its own -- like a file that needs to be downloaded by hand and placed in a particular location -- those steps should be laid out clearly at the beginning of the notebook.\n",
    "\n",
    "## Avoid Troublesome Elements\n",
    "\n",
    "### Don't hardcode passwords or access tokens\n",
    "Most of our work is done in public Github repositories.\n",
    "No authentication information should ever appear in a notebook.\n",
    "These values can be stored in environment variables on your local computer.\n",
    "\n",
    "### Do not hardocde values, especially late in the notebook\n",
    "If the analysis depends on particular choices of input values, those should be called out explicitly at the beginning of the notebook.\n",
    "(NB: We should explore ways to parameterize notebooks, [papermill](https://papermill.readthedocs.io/en/latest/) is one tool that does this).\n",
    "\n",
    "### Don't hardcode absolute file paths\n",
    "If anyone is going to be able to use the notebook, the files it uses will need to be stored somewhere that makes sense on both your and other computers.\n",
    "We assume that anyone using this template has the PUDL package installed, and has a local PUDL data management environment set up.\n",
    "  * Input data that needs to be stored on disk and accessed via a shared location should be saved under `<PUDL_IN>/data/local/<data_source>/`.\n",
    "  * Intermediate saved data products (e.g. a pickled result of a computationally intensive process) and results should be saved to a location relative to the notebook, and within the directory hierarchy of the repository that the notebook is part of.\n",
    "  \n",
    "### Don't require avoidable long-running computations\n",
    "Consider persisting to disk the results of computations that take more than a few minutes, if the outputs are small enough to be checked into the repository and shared with other users.\n",
    "Only require the expensive computation to be run if this pre-computed output is not available.\n",
    "\n",
    "### Don't litter\n",
    "Don't leave lots of additional code laying around, even commented out, \"just in case\" you want to look at it again later.\n",
    "Notebooks need to be relatively linear in the end, even though the thought processes and exploratory analyses that generate them may not be.\n",
    "Once you have a working analysis, either prune those branches, or encapsulate them as options within functions.\n",
    "\n",
    "### Don't load unneccesary libraries\n",
    "Only import libraries which are required by the notebook, to avoid unnecessary dependencies.\n",
    "If your analysis requires a new library that isn't yet part of the shared `pudl-dev` environment, add it to the `devtools/environment.yml` file so that others will get it when they update their environment.\n",
    "\n",
    "## Related Resources:\n",
    "Lots of these guidelines are taken directly from Emily Riederer's post: [RMarkdown Driven Development](https://emilyriederer.netlify.app/post/rmarkdown-driven-development/).\n",
    "For more in depth explanation of the motivations behind this layout, do go check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Import Libraries\n",
    "* Because it's very likely that you will be editing the PUDL Python packages or your own local module under development while working in the notebook, use the %autoreload magic with autoreload level 2 to ensure that any changes you've made in those files are always reflected in the code that's running in the notebook.\n",
    "* Put all import statements at the top of the notebook, so everyone can see what its dependencies are up front, and so that if an import is going to fail, it fails early, before the rest of the notebook is run.\n",
    "* Try to avoid importing individual functions and classes from deep within packages, as it may not be clear to other users where those elements came from, later in the notebook, and also to minimize the chance of namespace collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# 3rd party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sa\n",
    "\n",
    "# Local libraries\n",
    "import pudl\n",
    "import pudl.constants as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Display Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (10,4)\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Python Logging facilities\n",
    "* Using a logger from the beginning will make the transition into the PUDL package easier.\n",
    "* Creating a logging handler here will also allow you to see the logging output coming from PUDL and other underlying packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger=logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions\n",
    "In many cases, the eventual product of a notebook analysis is going to be the creation of new, reusable functions that are integrated into the underlying PUDL code. You should begin the process of accumulating and organizing those functions as soon as you notice repeated patterns in your code.\n",
    "* Functions should be used to encapsulate any potentially reusable code.\n",
    "* Functions should be defined immediately after the imports, to avoid accidental dependence on zombie variables that are defined further down in the code\n",
    "* While they may evolve over time, having brief docstrings explaining what they do will help others understand them.\n",
    "* If there's a particular type of plot or visualization that is made repeatedly in the notebook, it's good to parameterize and functionalize it here too, so that as you refine the presentation of the data and results, those improvements can be made in a single place, and shown uniformly throughout the notebook.\n",
    "* As these functions mature and become more general purpose tools, you will probably want to start migrating them into their own local module, under a `src` directory in the same directory as the notebook. You will want to import this module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy EIA 861 ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_etl_eia(eia_inputs, pudl_settings):\n",
    "    \"\"\"\n",
    "    This is a dummy function that runs the first part of the EIA ETL\n",
    "    process -- everything up until the entity harvesting begins. For\n",
    "    use in this notebook only.\n",
    "\n",
    "    \"\"\"\n",
    "    eia860_tables = eia_inputs[\"eia860_tables\"]\n",
    "    eia860_years = eia_inputs[\"eia860_years\"]\n",
    "    eia861_tables = eia_inputs[\"eia861_tables\"]\n",
    "    eia861_years = eia_inputs[\"eia861_years\"]\n",
    "    eia923_tables = eia_inputs[\"eia923_tables\"]\n",
    "    eia923_years = eia_inputs[\"eia923_years\"]\n",
    "\n",
    "    # generate CSVs for the static EIA tables, return the list of tables\n",
    "    #static_tables = _load_static_tables_eia(datapkg_dir)\n",
    "    \n",
    "    # For new Datastore args\n",
    "    pudl_in = pathlib.Path(pudl_settings['pudl_in'])\n",
    "    ds = pudl.workspace.datastore.Datastore(pudl_in, sandbox=True)\n",
    "    \n",
    "    # Extract EIA forms 923, 860\n",
    "    eia860_raw_dfs = pudl.extract.eia860.Extractor(ds).extract(eia860_years)\n",
    "    eia861_raw_dfs = pudl.extract.eia861.Extractor(ds).extract(eia861_years)\n",
    "    eia923_raw_dfs = pudl.extract.eia923.Extractor(ds).extract(eia923_years)\n",
    "\n",
    "    # Transform EIA forms 860, 861, 923\n",
    "    eia860_transformed_dfs = pudl.transform.eia860.transform(eia860_raw_dfs, eia860_tables=eia860_tables)\n",
    "    eia861_transformed_dfs = pudl.transform.eia861.transform(eia861_raw_dfs, eia861_tables=eia861_tables)\n",
    "    eia923_transformed_dfs = pudl.transform.eia923.transform(eia923_raw_dfs, eia923_tables=eia923_tables)\n",
    "\n",
    "    # create an eia transformed dfs dictionary\n",
    "    eia_transformed_dfs = eia860_transformed_dfs.copy()\n",
    "    eia_transformed_dfs.update(eia861_transformed_dfs.copy())\n",
    "    eia_transformed_dfs.update(eia923_transformed_dfs.copy())\n",
    "\n",
    "    # convert types..\n",
    "    eia_transformed_dfs = pudl.helpers.convert_dfs_dict_dtypes(eia_transformed_dfs, 'eia')\n",
    "\n",
    "    return eia860_raw_dfs, eia861_raw_dfs, eia923_raw_dfs, eia_transformed_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Notebook Parameters\n",
    "If there are overarching parameters which determine the nature of the analysis -- which US states to look at, which utilities are of interest, a particular start and end date -- state those clearly at the beginning of the analysis, so that they can be referred to by the rest of the notebook and easily changed if need be.\n",
    "* If the analysis depends on local (non-PUDL managed) datasets, define the paths to those data here.\n",
    "* If there are external URLs or other resource locations that will be accessed, define those here as well.\n",
    "* This is also where you should create your `pudl_settings` dictionary and connections to your local PUDL databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pudl_in': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR',\n",
       " 'data_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/data',\n",
       " 'settings_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/settings',\n",
       " 'pudl_out': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR',\n",
       " 'sqlite_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/sqlite',\n",
       " 'parquet_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/parquet',\n",
       " 'datapkg_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/datapkg',\n",
       " 'notebook_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/notebook',\n",
       " 'ferc1_db': 'sqlite:////Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/sqlite/ferc1.sqlite',\n",
       " 'pudl_db': 'sqlite:////Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/sqlite/pudl.sqlite'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Engine(sqlite:////Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/sqlite/ferc1.sqlite)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Engine(sqlite:////Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/sqlite/pudl.sqlite)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EIA861_YEARS = list(range(2001, 2019))\n",
    "pudl_settings = pudl.workspace.setup.get_defaults()\n",
    "display(pudl_settings)\n",
    "\n",
    "ferc1_engine = sa.create_engine(pudl_settings['ferc1_db'])\n",
    "display(ferc1_engine)\n",
    "\n",
    "pudl_engine = sa.create_engine(pudl_settings['pudl_db'])\n",
    "display(pudl_engine)\n",
    "\n",
    "pudl_out = pudl.output.pudltabl.PudlTabl(pudl_engine)\n",
    "\n",
    "\n",
    "# Is there other external data you need to pull in?\n",
    "# If so, put it in a (relatively) standard place on the filesystem.\n",
    "my_new_data_url = \"https://mynewdata.website.gov/path/to/new/data.csv\"\n",
    "my_new_datadir = pathlib.Path(pudl_settings[\"data_dir\"]) / \"local/new_data_source\"\n",
    "\n",
    "# Store API keys and other secrets in environment variables\n",
    "# and read them in here, if needed:\n",
    "# API_KEY_EIA = os.environ[\"API_KEY_EIA \"]\n",
    "# API_KEY_FRED = os.environ[\"API_KEY_FRED \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "* Given the above parameters and functions, it should now be possible to load the required data into local variables for further wrangling, analysis, and visualization.\n",
    "* If the data is not yet present on the machine of the person running the notebook, this step should also acquire the data from its original source, and place it in the appropriate location under `<PUDL_IN>/data/local/`.\n",
    "* If there are steps which have to be done manually here, put them first so that they fail first if the user hasn't read the instructions, and they can fix the situation before a bunch of other work gets done. Try to minimize the amount of work in the filesystem that has to be done manually though.\n",
    "* If this process takes a little while, don't be shy about producing `logging` output.\n",
    "* Using the `%%time` cell magic can also help users understand which pieces of work / data acquisition are hard:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EIA 861 (2010-2018)\n",
    "* Not yet fully integrated into PUDL\n",
    "* Post-transform harvesting process isn't compatible w/ EIA 861 structure\n",
    "* Only getting the `sales_eia861`, `balancing_authority_eia861`, and `service_territory_eia861` tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Already Transformed EIA 861 DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No years given. Not extracting eia860 spreadsheet data.\n",
      "Extracting eia861 spreadsheet data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aesharpe/Desktop/Work/Catalyst_Coop/pudl/src/pudl/extract/eia861.py:39: UserWarning: Integration of EIA 861 into PUDL is still experimental and incomplete.\n",
      "The data has not yet been validated, and the structure may change.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "No years given. Not extracting eia923 spreadsheet data.\n",
      "No raw EIA 860 dataframes found. Not transforming EIA 860.\n",
      "Transforming raw EIA 861 DataFrames for service_territory_eia861 concatenated across all years.\n",
      "Assigned state FIPS codes for 100.00% of records.\n",
      "Assigned county FIPS codes for 99.64% of records.\n",
      "Transforming raw EIA 861 DataFrames for balancing_authority_eia861 concatenated across all years.\n",
      "Started with 37622 missing BA Codes out of 38882 records (96.76%)\n",
      "Ended with 12674 missing BA Codes out of 38882 records (32.60%)\n",
      "Transforming raw EIA 861 DataFrames for sales_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Sales table.\n",
      "Dropped 0 duplicate records from EIA 861 Demand Response table, out of a total of 301045 records (0.0000% of all records). \n",
      "Performing value transformations on EIA 861 Sales table.\n",
      "Transforming raw EIA 861 DataFrames for advanced_metering_infrastructure_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Advanced Metering Infrastructure table.\n",
      "Transforming raw EIA 861 DataFrames for demand_response_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Demand Response table.\n",
      "Dropped 0 duplicate records from EIA 861 Demand Response table, out of a total of 10644 records (0.0000% of all records). \n",
      "Performing value transformations on EIA 861 Demand Response table.\n",
      "Transforming raw EIA 861 DataFrames for demand_side_management_eia861 concatenated across all years.\n",
      "Transforming raw EIA 861 DataFrames for distributed_generation_eia861 concatenated across all years.\n",
      "Converting pct values into mw values for distributed generation misc table\n",
      "Converting pct values into mw values for distributed generation tech table\n",
      "Tidying Distributed Generation Tech Table\n",
      "Tidying Distributed Generation Fuel Table\n",
      "Transforming raw EIA 861 DataFrames for distribution_systems_eia861 concatenated across all years.\n",
      "Transforming raw EIA 861 DataFrames for dynamic_pricing_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Dynamic Pricing table.\n",
      "Performing value transformations on EIA 861 Dynamic Pricing table.\n",
      "Transforming raw EIA 861 DataFrames for energy_efficiency_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Energy Efficiency table.\n",
      "Transforming the EIA 861 Energy Efficiency table.\n",
      "Transforming raw EIA 861 DataFrames for green_pricing_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Green Pricing table.\n",
      "Performing value transformations on EIA 861 Green Pricing table.\n",
      "Transforming raw EIA 861 DataFrames for mergers_eia861 concatenated across all years.\n",
      "Transforming raw EIA 861 DataFrames for net_metering_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Net Metering table.\n",
      "Transforming raw EIA 861 DataFrames for non_net_metering_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Non Net Metering table.\n",
      "Transforming raw EIA 861 DataFrames for operational_data_eia861 concatenated across all years.\n",
      "The following reported NERC regions are not currently recognized and become UNK values: ['CLECO']\n",
      "Transforming raw EIA 861 DataFrames for reliability_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Reliability table.\n",
      "Transforming raw EIA 861 DataFrames for utility_data_eia861 concatenated across all years.\n",
      "The following reported NERC regions are not currently recognized and become UNK values: ['SASKATCHWA']\n",
      "Tidying the EIA 861 Utility Data tables.\n",
      "Building an EIA 861 BA-Util-State association table.\n",
      "Building an EIA 861 Util-State-Date association table.\n",
      "Completing normalization of balancing_authority_eia861.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aesharpe/miniconda3/envs/pudl-dev/lib/python3.8/site-packages/pandas/core/missing.py:47: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask = arr == x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No raw EIA 923 DataFrames found. Not transforming EIA 923.\n",
      "CPU times: user 2min 31s, sys: 5.47 s, total: 2min 36s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eia_inputs = {\n",
    "    \"eia860_years\": [],\n",
    "    \"eia860_tables\": pudl.constants.pudl_tables[\"eia860\"],\n",
    "    \"eia861_years\": EIA861_YEARS,\n",
    "    \"eia861_tables\": pudl.constants.pudl_tables[\"eia861\"],\n",
    "    \"eia923_years\": [],\n",
    "    \"eia923_tables\": pudl.constants.pudl_tables[\"eia923\"],\n",
    "}\n",
    "eia860_raw_dfs, eia861_raw_dfs, eia923_raw_dfs, eia_transformed_dfs = test_etl_eia(eia_inputs=eia_inputs, pudl_settings=pudl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check Data\n",
    "If there's any validation that can be done on the data which you've loaded to flag if/when it is inappropriate for the analysis that follows, do it here. If you find the data is unusable, use `assert` statements or `raise` Exceptions to stop the notebook from proceeding, and indicate what the problem is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: \n",
    "    * compare totals col - result: total_capacity col is more accurate but need total_capacity_mw for conversion of pct to       mw\n",
    "    * ensure that col breakdown is acurate:\n",
    "        * that all columns are accounted for\n",
    "        * that the \"tech\" cols are actually components that sum to the total\n",
    "        * check on the wierd extra cols that are the \"half\" components.\n",
    "    * deal with / record duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Data Wrangling\n",
    "Once all of the data is loaded and looks like it's in good shape, do any initial wrangling that's specific to this particular analysis. This should mostly make use of the higher level functions which were defined above. If this step takes a while, don't be shy about producing `logging` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_ee = eia_transformed_dfs['energy_efficiency_eia861']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming the EIA 861 Energy Efficiency table.\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Transform Values:\n",
    "# * Turn 1000s of dollars back into dollars\n",
    "# * Get rid of website column\n",
    "###########################################################################\n",
    "\n",
    "logger.info(\"Transforming the EIA 861 Energy Efficiency table.\")\n",
    "\n",
    "transformed_ee = (\n",
    "    tidy_ee.assign(\n",
    "        customer_incentives_incremental_cost=lambda x: (\n",
    "            x.customer_incentives_incremental_cost * 1000),\n",
    "        customer_incentives_incremental_life_cycle_cost = lambda x: (\n",
    "            x.customer_incentives_incremental_life_cycle_cost * 1000),\n",
    "        customer_other_costs_incremental_life_cycle_cost = lambda x: (\n",
    "            x.customer_other_costs_incremental_life_cycle_cost * 1000),\n",
    "        other_costs_incremental_cost = lambda x: (\n",
    "            x.other_costs_incremental_cost * 1000),\n",
    "    ).drop(['website'], axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utility_id_eia',\n",
       " 'state',\n",
       " 'balancing_authority_code_eia',\n",
       " 'report_date',\n",
       " 'utility_name_eia',\n",
       " 'customer_class',\n",
       " 'customer_incentives_incremental_cost',\n",
       " 'customer_incentives_incremental_life_cycle_cost',\n",
       " 'customer_other_costs_incremental_life_cycle_cost',\n",
       " 'incremental_energy_savings_mwh',\n",
       " 'incremental_life_cycle_energy_savings_mwh',\n",
       " 'incremental_life_cycle_peak_reduction_mwh',\n",
       " 'incremental_peak_reduction_mw',\n",
       " 'other_costs_incremental_cost',\n",
       " 'weighted_average_life_years']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_ee.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Match Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fuzzy_match(df, col1, col2):\n",
    "    # Define base comparison df \n",
    "    eia_base_df = (\n",
    "        pudl_out.utils_eia860()\n",
    "        #[['report_date', 'utility_id_eia', 'utility_id_pudl', 'utility_name_eia', 'state']]\n",
    "        .assign(utility_id_eia=lambda x: x.utility_id_eia.astype('Int64'))\n",
    "    )\n",
    "    # Shorten df\n",
    "    #df = df[['utility_id_eia', 'state', 'utility_name_eia', 'report_date']]\n",
    "    \n",
    "    # Merge dfs together \n",
    "    df_merge = (\n",
    "        pd.merge(eia_base_df, df, on=['utility_id_eia', 'state'], how='outer', suffixes=('_base', '_861'))\n",
    "        .set_index(['utility_id_eia', 'state'])\n",
    "        .assign(utility_name_eia_base=lambda x: x.utility_name_eia_base.astype('string'),\n",
    "                utility_name_eia_861=lambda x: x.utility_name_eia_861.astype('string'))\n",
    "    )\n",
    "    \n",
    "    # Only run where both columns for utility name have values\n",
    "    df_merge_no_null = (\n",
    "        df_merge.loc[\n",
    "            (df_merge[col1].notnull()) \n",
    "            & (df_merge[col2].notnull())].copy()\n",
    "    )\n",
    "    \n",
    "    # Fuzzy match\n",
    "    df_merge_no_null['fuzzy'] = (\n",
    "        df_merge_no_null.apply(lambda x: fuzz.ratio(x.utility_name_eia_base, x.new_parent),axis=1)\n",
    "    )\n",
    "#     df_merge_no_null = (\n",
    "#         df_merge_no_null.assign(fuzzy=lambda x: fuzz.ratio(x[col1], x[col2])))\n",
    "    \n",
    "    #df_merge_no_null.loc[:,'fuzzy'] = fuzzy_series\n",
    "    df_merge_no_null = df_merge_no_null.sort_values('fuzzy')\n",
    "    \n",
    "    return df_merge_no_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSM Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm = eia861_raw_dfs['demand_side_management_eia861'].copy()\n",
    "#dsm.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_cols = ['utility_id_eia', 'state', 'ba_code', 'report_year']\n",
    "\n",
    "dsm_test = dsm[['utility_id_eia', 'state', 'ba_code', 'report_year', \n",
    "                'residential_annual_load_management_energy_effects ', \n",
    "                'commercial_annual_load_management_energy_effects ', \n",
    "                'industrial_annual_load_management_energy_effects ',\n",
    "                'transportation_annual_load_management_energy_effects ',\n",
    "                'total_annual_load_management_energy_effects ',\n",
    "]]\n",
    "\n",
    "dr_test = raw_dr_eia861[['utility_id_eia', 'state', 'balancing_authority_code_eia', 'report_year',\n",
    "                         'residential_energy_savings_mwh',\n",
    "                         'commercial_energy_savings_mwh',\n",
    "                         'industrial_energy_savings_mwh',\n",
    "                         'transportation_energy_savings_mwh',\n",
    "                         'total_energy_savings_mwh'\n",
    "]].rename(columns={'balancing_authority_code_eia': 'ba_code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.merge(dsm_test, dr_test, on=idx_cols, how='outer')\n",
    "merge = pudl.helpers.oob_to_nan(merge, \n",
    "                       ['residential_annual_load_management_energy_effects ', \n",
    "                        'commercial_annual_load_management_energy_effects ', \n",
    "                        'industrial_annual_load_management_energy_effects ',\n",
    "                        'transportation_annual_load_management_energy_effects ',\n",
    "                        'residential_energy_savings_mwh',\n",
    "                        'commercial_energy_savings_mwh',\n",
    "                        'industrial_energy_savings_mwh',\n",
    "                        'transportation_energy_savings_mwh',\n",
    "                        'total_energy_savings_mwh',\n",
    "                        'total_annual_load_management_energy_effects '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_col = 'residential_energy_savings_mwh'\n",
    "dsm_col = 'residential_annual_load_management_energy_effects '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b335c7e0fa14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmerge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'report_year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2012\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2014\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmerge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdr_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#merge = merge.loc[merge[dsm_col].notna()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merge' is not defined"
     ]
    }
   ],
   "source": [
    "merge = merge.loc[merge['report_year'].isin(range(2012,2014))]\n",
    "merge = merge.loc[merge[dr_col].notna()]\n",
    "#merge = merge.loc[merge[dsm_col].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.plot.scatter('residential_annual_load_management_energy_effects ', 'residential_energy_savings_mwh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_dr_eia861.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Visualization\n",
    "* Now that you've got the required data in a usable form, you can tell the story of your analysis through a mix of visualizations, and further data wrangling steps.\n",
    "* This narrative should be readable, with figures that have titles, legends, and labeled axes as appropriate so others can understand what you're showing them.\n",
    "* The code should be concise and make use of the parameters and functions which you've defined above when possible. Functions should contain comprehensible chunks of work that make sense as one step in the story of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
