{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install PUDL\n",
    "* Until we get our custom Docker image built, PUDL needs to be installed in your user environment each session.\n",
    "* If you are using this notebook on the Catalyst JupyterHub, and this is the first notebook you've used this session, then uncomment the commands in the following cell and run it before anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install --yes --quiet python-snappy\n",
    "#!pip install --upgrade pip\n",
    "#!pip install --quiet git+https://github.com/catalyst-cooperative/pudl.git@jupyterhub-beta\n",
    "#!cp ~/shared/shared-pudl.yml ~/.pudl.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Working with EPA CEMS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEMS or <a href='https://www.epa.gov/emc/emc-continuous-emission-monitoring-systems'>**Continusous Emissions Monitoring Systems**</a> are used to track power plant's compliance with EPA emission standards. Among the data are hourly measurements of SO2, CO2, and NOx emissions associated with a given plant. The EPA's <a href='https://www.epa.gov/airmarkets'>Clean Air Markets Division</a> has collected CEMS data stretching back to 1990 and publicized it in their <a href='https://ampd.epa.gov/ampd/'>data portal</a>. Combinging the CEMS data with EIA and FERC data provides access to greater and more specific information about utilities and generation facilities. This notebook provides a glimpse into the analysis potential of CEMS data usage and integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: This Notebook presuposes access to the parquet files in which the CEMS data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUDL Notebook Setup\n",
    "\n",
    "The following packages enable interaction with the CEMS dataset through pudl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import logging\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# 3rd party libraries\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "\n",
    "# Local libraries\n",
    "import pudl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger=logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pudl_settings = pudl.workspace.setup.get_defaults()\n",
    "#display(pudl_settings)\n",
    "\n",
    "ferc1_engine = sa.create_engine(pudl_settings['ferc1_db'])\n",
    "#display(ferc1_engine)\n",
    "\n",
    "pudl_engine = sa.create_engine(pudl_settings['pudl_db'])\n",
    "#display(pudl_engine)\n",
    "\n",
    "#pudl_engine.table_names()\n",
    "pudl_out = pudl.output.pudltabl.PudlTabl(pudl_engine, freq='AS') #annual frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigger Data in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CEMS dataset is enormous! It contains hourly operations and emissions data for thousands of power plants between 1995 and 2019. The full dataset is close to a billion rows and takes up 100 GB of space uncompressed, which means it will not all fit in memory on your laptop at the same time. One of the best tools for dealing with larger-than-memory data analysis in Python is the [Dask](https://dask.org) library, which extends Pandas, while maintaining a pretty similiar interface. We'll be using some of Dask's features below to work with this larger dataset.\n",
    "\n",
    "Additional resources you might want to check out:\n",
    "* [Scaling Pandas](https://tomaugspurger.github.io/modern-8-scaling.html) is a brief introduction to working with larger datasets using Dask.\n",
    "* [How to learn Dask in 2020](https://coiled.io/blog/how-to-learn-dask-in-2020/) is a more extensive collection of self-guided educational resources.\n",
    "\n",
    "We store the EPA CEMS data in [Apache Parquet files](https://parquet.apache.org/), which are optimized for fast reading of individual columns of tabular data. We partition the dataset by state and year, to make it easy to read in only the chunks that you actually need.\n",
    "\n",
    "One of the main features of Dask is \"lazy execution\" -- it doesn't read data or do any computation until it has to, or you explicitly tell it to. Instead, it allows you to build of a series of computational instructions, and then it intelligently looks at those instructions and executes them so as to minimize memory utilization and/or parallelize the tasks if possible.\n",
    "\n",
    "For example, the following cell will \"read\" in the full EPA CEMS dataset almost instantly... but that's because no data has been read off of disk yet. The instructions for reading off of the disk have been added to the list of things the Dask dataframe will do when you actually as it for a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epacems_path = pathlib.Path(pudl_settings[\"parquet_dir\"]) / \"epacems\"\n",
    "epacems_dd = dd.read_parquet(epacems_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, some information about the dataframe is available at this point -- including the names of the columns and their data types -- because that information is stored in the Parquet file metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epacems_dd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know anything about the contents of the data itself, some data will have to be read in, and some computation performed. Before any aggregations or calculations or selections have happened, you can ask how many rows there are in the whole parquet dataset, and it will be returned relatively quickly, since that information is also stored in the metadata. This would behave differently later on, since after you've told the dataframe to do some computation, you'd be asking for the length of the **result of that computation** not the original inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(epacems_dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing a Subset of EPA CEMS Data\n",
    "\n",
    "One strategy for working with a large dataset is to only look at small subsets of it in any given analysis.\n",
    "\n",
    "You can point Dask at a large parquet dataset like EPA CEMS and tell it to read in only a subset of the columns, and the select a subset of the rows based on some criteria of interest. So long as the collection of rows and columns you've specified end up being smaller than the available memory on you computer, this operation will succeed. However, depending on how fast your disk is, it may still take longer than you would like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Filtering\n",
    "The following statement will tell Dask where the dataset is, and describe a subset of the data that should be returned as a pandas dataframe (only data about power plants in Colorado in 2019). This should be small enough to fit in memory, but it won't actually read the data in yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_epacems_dd = (\n",
    "    dd.read_parquet(epacems_path)\n",
    "    .query(\"year==2019\")\n",
    "    .query(\"state=='CO'\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're satisfied that this is the data we want, then we can go ahead and ask Dask to read it in and give us a pandas dataframe back. This will allows us to search through a billion rows of data on disk and select only the small subset that we are interested in without blowing up our memory usage. This is very flexible -- you can select on whatever criteria you want -- but it's not very efficient. It will still take maybe 5 minutes on a laptop with a fast SSD (uncomment it if you want to actually give this a try...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#slow_epacems_df = slow_epacems_dd.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter using Parquet partitions\n",
    "To make operations like this faster and easier, we have pre-partitioned the EPA CEMS Parquet dataset by `state` and `year` -- there's a separate parquet file that contains each state's data for each year, in a directory structure that many tools for working with Parquet files understand. This means you can tell Dask that there are only certain files it needs to look in for the data you're interested in. Rather than scanning through 100 GB of data across 1300 different files, it can just read data from the single file that contains the 2019 data for Colorado. On a computer with a fast SSD, this should takes about a fraction of a second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epacems_dd = dd.read_parquet(\n",
    "    epacems_path,\n",
    "    filters=[[(\"state\", \"=\", \"CO\"), (\"year\", \"=\", 2019)]],\n",
    ")\n",
    "epacems_df = epacems_dd.compute()\n",
    "epacems_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Partition Filtering Shortcut\n",
    "Unfortunately, the nested list of lists of boolean predicates used to specify the filters (known as [disjunctive normal form](https://en.wikipedia.org/wiki/Disjunctive_normal_form) is a little unwieldy, so we have some helper functions that can construct them for you. `pudl.output.epacems.year_state_filter()` will give you a filter that selects the data for all combinations of the years and states you specify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_filter = pudl.output.epacems.year_state_filter(years=[2018, 2019], states=[\"CO\", \"CA\"])\n",
    "demo_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epacems_dd = dd.read_parquet(\n",
    "    epacems_path,\n",
    "    filters=demo_filter\n",
    ")\n",
    "epacems_df = epacems_dd.compute()\n",
    "epacems_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting columns\n",
    "Often you will only be interested in a certain subset of the columns stored in a parquet dataset. Parquet files are organized internally to make it very efficient to only select certain columns, and the `read_parquet()` method makes this easy. Say you're only interested in power outputs, fuel consumed, and CO2 emissions, but not traditional air pollutants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ghg_cols = [\n",
    "    \"state\",\n",
    "    \"year\",\n",
    "    \"operating_datetime_utc\",\n",
    "    \"plant_id_eia\",\n",
    "    \"facility_id\",\n",
    "    \"unit_id_epa\",\n",
    "    \"unitid\",\n",
    "    \"operating_time_hours\",\n",
    "    \"gross_load_mw\",\n",
    "    \"heat_content_mmbtu\",\n",
    "    \"co2_mass_tons\",\n",
    "]\n",
    "epacems_dd = dd.read_parquet(\n",
    "    epacems_path,\n",
    "    filters=demo_filter,\n",
    "    columns=ghg_cols,\n",
    ")\n",
    "epacems_df = epacems_dd.compute()\n",
    "epacems_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining these selection tools\n",
    "We can combine these methods to select a small subset of the large dataset very quickly. Let's just load data related to Xcel Energy's Comanche coal plant in Colorado. Since the EPA CEMS data only contains the EIA Plant ID, we need to look that up first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comanche_colorado = (\n",
    "    pudl_out.plants_eia860()\n",
    "    .query(\"plant_name_eia=='Comanche'\")\n",
    "    .query(\"state=='CO'\")\n",
    "    .loc[:,[\"report_date\", \"plant_id_eia\", \"plant_name_eia\", \"utility_id_eia\", \"city\", \"state\"]]\n",
    "    .merge(pudl_out.utils_eia860()[[\"utility_id_eia\", \"utility_name_eia\", \"report_date\"]])\n",
    ")\n",
    "comanche_colorado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, we can see that Comanche has `plant_id_eia == 470` and we already know that it's in Colorado. So let's load all the years of data from Colorado, but only for Comanche, and calculate these additional values\n",
    "* Gross Generation (MWh)\n",
    "* Heat Rate (mmBTU / MWh)\n",
    "* Gross CO2 intensity (tons / MWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "only_colorado = pudl.output.epacems.year_state_filter(states=[\"CO\"])\n",
    "comanche_dd = (\n",
    "    dd.read_parquet(\n",
    "        epacems_path,\n",
    "        filters=only_colorado,\n",
    "        columns=ghg_cols,\n",
    "    )\n",
    "    .query(\"plant_id_eia==470\")\n",
    ")\n",
    "comanche_df = (\n",
    "    comanche_dd.compute()\n",
    "    .assign(\n",
    "        gross_generation_mwh=lambda x: x.operating_time_hours * x.gross_load_mw,\n",
    "        heat_rate_mmbtu_mwh=lambda x: x.heat_content_mmbtu / x.gross_generation_mwh,\n",
    "        gross_co2_intensity=lambda x: x.co2_mass_tons / x.gross_generation_mwh\n",
    "    )\n",
    ")\n",
    "comanche_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has selected almost 500,000 rows and 50 MB of data out of ~1 billion rows and ~100 GB in a few seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comanche_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Plant Operations Through Time\n",
    "Now that we've got a managable dataframe, we can visualize what's inside it! Here's the heat content of the fuel consumed each hour for the last 25 years in the three Comanche coal units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuel Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_ids = comanche_df.unitid.unique()\n",
    "fig, axs = plt.subplots(nrows=len(unit_ids), ncols=1, sharex=True, figsize=(20,15))\n",
    "for n, unitid in enumerate(comanche_df.unitid.unique()):\n",
    "    axs[n].scatter(\n",
    "        comanche_df.query(\"unitid==@unitid\").operating_datetime_utc,\n",
    "        comanche_df.query(\"unitid==@unitid\").heat_content_mmbtu,\n",
    "        s=0.2, alpha=0.1,\n",
    "    )\n",
    "    axs[n].set_ylim(-100,10_000)\n",
    "    axs[n].set_title(f\"Comanche Unit {unitid}\")\n",
    "    axs[n].set_ylabel(\"Fuel consumed [mmbtu / hr]\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_ids = comanche_df.unitid.unique()\n",
    "fig, axs = plt.subplots(nrows=len(unit_ids), ncols=1, sharex=True, figsize=(20,15))\n",
    "for n, unitid in enumerate(comanche_df.unitid.unique()):\n",
    "    axs[n].scatter(\n",
    "        comanche_df.query(\"unitid==@unitid\").operating_datetime_utc,\n",
    "        comanche_df.query(\"unitid==@unitid\").heat_rate_mmbtu_mwh,\n",
    "        s=0.2, alpha=0.1,\n",
    "    )\n",
    "    axs[n].set_ylim(-1,20)\n",
    "    axs[n].set_title(f\"Comanche Unit {unitid}\")\n",
    "    axs[n].set_ylabel(\"Heat Rate [mmBTU / MWh]\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carbon Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_ids = comanche_df.unitid.unique()\n",
    "fig, axs = plt.subplots(nrows=len(unit_ids), ncols=1, sharex=True, figsize=(20,15))\n",
    "for n, unitid in enumerate(comanche_df.unitid.unique()):\n",
    "    axs[n].scatter(\n",
    "        comanche_df.query(\"unitid==@unitid\").operating_datetime_utc,\n",
    "        comanche_df.query(\"unitid==@unitid\").gross_co2_intensity,\n",
    "        s=0.2, alpha=0.1,\n",
    "    )\n",
    "    axs[n].set_ylim(0.5,1.5)\n",
    "    axs[n].set_title(f\"Comanche Unit {unitid}\")\n",
    "    axs[n].set_ylabel(\"Gross CO2 Intensity [tons / MWh]\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_ids = comanche_df.unitid.unique()\n",
    "fig, axs = plt.subplots(nrows=len(unit_ids), ncols=1, sharex=True, figsize=(20,15))\n",
    "for n, unitid in enumerate(comanche_df.unitid.unique()):\n",
    "    axs[n].scatter(\n",
    "        comanche_df.query(\"unitid==@unitid\").operating_datetime_utc,\n",
    "        comanche_df.query(\"unitid==@unitid\").gross_load_mw,\n",
    "        s=0.2, alpha=0.1,\n",
    "    )\n",
    "    axs[n].set_ylim(-100,1000)\n",
    "    axs[n].set_title(f\"Comanche Unit {unitid}\")\n",
    "    axs[n].set_ylabel(\"Gross Load [MW]\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating / Downsampling EPA CEMS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than selecting a small subset of a large dataset at its full resolution, we can aggregate records to reduce large datasets to a managable size. Now that you know what's available, you can pick the columns you'd like to work with, and aggregate rows if necessary. \n",
    "\n",
    "**Note** that in CEMS the `state` and `measurement_code` columns are categorical datatypes, meaning that they will overwhelm your computer's memory if included in the list of columns you'd like to groupby. In pandas, this is solved by including the statement `observed=True` in the groupby, but with Dask we'll solve this by changing the datatype to string. As mentioned previously, the dataset is very large. If the Dask dataframe you construct is too similar to the original dataset -- imagine the example below without the `groupby` -- the client will be unable to load it in pandas. The dataset below should load in a couple of minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Plant-level Emissions for Two Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Prepare CEMS data\n",
    "\n",
    "# A list of the columns you'd like to include in your analysis\n",
    "my_cols = [\n",
    "    'year',\n",
    "    'plant_id_eia', \n",
    "    'unitid',\n",
    "    'so2_mass_lbs',\n",
    "    'nox_mass_lbs',\n",
    "    'co2_mass_tons'\n",
    "]\n",
    "\n",
    "# Select emissions data are grouped by state, plant_id and unit_id\n",
    "# Remember to change the datatype for 'state' from category to string\n",
    "my_cems_dd = (\n",
    "    dd.read_parquet(epacems_path, columns=my_cols)\n",
    "    .astype({\n",
    "        'year': int,\n",
    "    })\n",
    "    .groupby(['year', 'plant_id_eia'])[[\n",
    "        'so2_mass_lbs',\n",
    "        'nox_mass_lbs',\n",
    "        'co2_mass_tons'\n",
    "    ]]\n",
    "    .sum()\n",
    ").reset_index()\n",
    "cems_byplant = my_cems_dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare EIA data for integration with CEMS\n",
    "plants_df = (\n",
    "    pudl_out.plants_eia860()\n",
    "    .loc[:, [\"plant_id_eia\", \"report_date\", \"plant_name_eia\", \"state\", \"longitude\", \"latitude\" ]]\n",
    "    .assign(year=lambda x: x.report_date.dt.year)\n",
    "    .drop(\"report_date\", axis=\"columns\")\n",
    "    .pipe(pudl.helpers.oob_to_nan, [\"latitude\"], lb=-90.0, ub=90.0)\n",
    "    .pipe(pudl.helpers.oob_to_nan, [\"longitude\"], lb=-180.0, ub=180.0)\n",
    "    .dropna()\n",
    "    .query(\"state not in ('AK', 'HI')\")\n",
    "    .query(\"latitude < 50.0\")\n",
    ")\n",
    "\n",
    "# Merge EIA plant data with CEMS aggregated plant data\n",
    "merged_cems_df = (\n",
    "    plants_df.merge(\n",
    "        cems_byplant,\n",
    "        on=[\"year\", \"plant_id_eia\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Make a geodataframe for mapping out of the merged plant data\n",
    "cems_gdf = (\n",
    "    gpd.GeoDataFrame(\n",
    "        merged_cems_df,\n",
    "        geometry=gpd.points_from_xy(merged_cems_df.longitude, merged_cems_df.latitude),\n",
    "        crs=\"EPSG:4326\",  # Geographic coordinates: Latitude / Longitude in degrees.\n",
    "    )\n",
    "    .drop([\"latitude\", \"longitude\"], axis=\"columns\")\n",
    "    .to_crs(\"EPSG:3857\")\n",
    ")\n",
    "cems_gdf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cems_gdf[cems_gdf['year']==2009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_point_source_emissions(gdf, year, states=None, scaling=1e4, alpha=0.25):\n",
    "    \"\"\"Map of emissions by plant coordinates in proportion to their annual amount.\"\"\"\n",
    "    import contextily as ctx\n",
    "    gdf_to_plot = gdf.query(\"year==@year\")\n",
    "    if states is not None:\n",
    "        gdf_to_plot = gdf_to_plot[gdf_to_plot.state.isin(states)]\n",
    "        \n",
    "    ax = gdf_to_plot.plot(\n",
    "        figsize=(40,20),\n",
    "        color=\"black\",\n",
    "        alpha=alpha,\n",
    "        markersize=gdf_to_plot.co2_mass_tons / scaling,\n",
    "    )\n",
    "    ax.set_title(f'CO2 Emissions {year}', fontsize=50)\n",
    "    ax.set_axis_off()\n",
    "    ctx.add_basemap(ax)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "map_point_source_emissions(cems_gdf, 2009, states=None, scaling=1e4, alpha=0.5)\n",
    "map_point_source_emissions(cems_gdf, 2019, states=None, scaling=1e4, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the difference. Of the plants that opperated both in 2009 and 2019, the red shows where there was an increase in CO2 emissions and the green shows where there was a decrease in CO2 emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emissions_difference(df):\n",
    "    \"\"\"Calculate the difference in emission between two years\"\"\"\n",
    "    df['year_diff'] = (\n",
    "        df.groupby('plant_id_eia')['co2_mass_tons']\n",
    "        .diff()\n",
    "    )\n",
    "    df = df.dropna()\n",
    "\n",
    "    #pos_df = df[df['co2_mass_tons']>0].copy()\n",
    "    pos_df = df.loc[df.co2_mass_tons>=0]\n",
    "    neg_df = (\n",
    "        df.loc[df.year_diff<0]\n",
    "        .assign(year_diff=lambda x: abs(x.year_diff))\n",
    "    )\n",
    "    \n",
    "    return pos_df, neg_df\n",
    "\n",
    "def map_emissions_difference(gdf, year_old, year_new, states=None, scaling=1e4, alpha=0.25):\n",
    "    \"\"\"Map of emissions by plant coordinates in proportion to their annual amount.\"\"\"\n",
    "    import contextily as ctx\n",
    "    gdf_to_plot = gdf.query(\"year==[@year_old, @year_new]\")\n",
    "    if states is not None:\n",
    "        gdf_to_plot = gdf_to_plot[gdf_to_plot.state.isin(states)]\n",
    "        \n",
    "    # Calculate emissions difference between years    \n",
    "    pos_df, neg_df = calculate_emissions_difference(gdf_to_plot)\n",
    "    \n",
    "    # Plot figure\n",
    "    fig, ax = plt.subplots(figsize=(40, 20))\n",
    "    pos_df.plot(\n",
    "        ax=ax,\n",
    "        color=\"green\",\n",
    "        alpha=alpha,\n",
    "        markersize=pos_df.year_diff / scaling,\n",
    "    )\n",
    "    neg_df.plot(\n",
    "        ax=ax,\n",
    "        color=\"red\",\n",
    "        alpha=alpha,\n",
    "        markersize=neg_df.year_diff / scaling,\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(f'CO2 Emissions Difference {year_old} - {year_new}', fontsize=50)\n",
    "    ctx.add_basemap(ax)\n",
    "    plt.show()\n",
    "        \n",
    "    return plt.show()\n",
    "\n",
    "map_emissions_difference(cems_gdf, 2009, 2019, states=None, scaling=1e4, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
